{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40571452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0f7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully!\n",
      "Output directory: /Users/yashbansal/Documents/cdc/Approach_3/outputs\n",
      "Explainability directory: /Users/yashbansal/Documents/cdc/Approach_3/Explainability\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    BASE_DIR = '/Users/yashbansal/Documents/cdc/Approach_3'\n",
    "    DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "    PROCESSED_DIR = os.path.join(DATA_DIR, 'processed')\n",
    "    RAW_DIR = os.path.join(DATA_DIR, 'raw')\n",
    "    TEST_IMG_DIR = os.path.join(DATA_DIR, 'test_data_images')\n",
    "    \n",
    "    # Image paths - Training images (used for both train and val)\n",
    "    TRAIN_IMG_19 = os.path.join(RAW_DIR, 'train_images_19')\n",
    "    TRAIN_IMG_20 = os.path.join(RAW_DIR, 'train_images_20')\n",
    "    \n",
    "    # Test images\n",
    "    TEST_IMG_19 = os.path.join(TEST_IMG_DIR, 'test_images_19')\n",
    "    TEST_IMG_20 = os.path.join(TEST_IMG_DIR, 'test_images_20')\n",
    "    \n",
    "    # Output paths\n",
    "    OUTPUT_DIR = os.path.join(BASE_DIR, 'outputs')\n",
    "    EXPLAINABILITY_DIR = os.path.join(BASE_DIR, 'Explainability')\n",
    "    MODEL_SAVE_PATH = os.path.join(OUTPUT_DIR, 'best_model.pth')\n",
    "    PREDICTIONS_PATH = os.path.join(OUTPUT_DIR, 'test_predictions.csv')\n",
    "    \n",
    "    # Model architecture\n",
    "    IMG_SIZE = 224  # Resize from 1280 to 224\n",
    "    ORIGINAL_IMG_SIZE = 1280\n",
    "    \n",
    "    # Tabular Transformer\n",
    "    TAB_D_MODEL = 128\n",
    "    TAB_NHEAD = 4\n",
    "    TAB_NUM_LAYERS = 2\n",
    "    TAB_OUTPUT_DIM = 128\n",
    "    \n",
    "    # Cross-modal attention\n",
    "    ATTENTION_DIM = 256\n",
    "    ATTENTION_HEADS = 4\n",
    "    \n",
    "    # Fusion MLP\n",
    "    FUSION_HIDDEN = [768, 384, 128]\n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 40\n",
    "    LR_MAIN = 3e-4\n",
    "    LR_BACKBONE = 4e-5\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    WARMUP_EPOCHS = 5\n",
    "    EARLY_STOPPING_PATIENCE = 15\n",
    "    \n",
    "    # Loss\n",
    "    HUBER_DELTA = 1.0\n",
    "    \n",
    "    # Freeze 75% of ResNet, unfreeze 25%\n",
    "    FREEZE_PERCENT = 0.75\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.EXPLAINABILITY_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(config.EXPLAINABILITY_DIR, 'validation'), exist_ok=True)\n",
    "os.makedirs(os.path.join(config.EXPLAINABILITY_DIR, 'test'), exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Output directory: {config.OUTPUT_DIR}\")\n",
    "print(f\"Explainability directory: {config.EXPLAINABILITY_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175345dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 12967\n",
      "Validation samples: 3242\n",
      "Test samples: 5404\n",
      "\n",
      "Train columns: ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_living_log', 'sqft_lot_log', 'sqft_above_log', 'sqft_basement_log', 'sqft_living15_log', 'sqft_lot15_log', 'property_age', 'was_renovated', 'years_since_renovation', 'has_basement', 'total_rooms', 'has_view', 'is_luxury', 'distance_from_seattle', 'price_log', 'price', 'id', 'lat', 'long', 'image_exists', 'image_path']\n",
      "\n",
      "Test columns: ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_living_log', 'sqft_lot_log', 'sqft_above_log', 'sqft_basement_log', 'sqft_living15_log', 'sqft_lot15_log', 'property_age', 'was_renovated', 'years_since_renovation', 'has_basement', 'total_rooms', 'has_view', 'is_luxury', 'distance_from_seattle', 'id', 'lat', 'long', 'image_path', 'image_exists']\n",
      "\n",
      "Train head:\n",
      "   bedrooms  bathrooms    floors  waterfront      view  condition     grade  \\\n",
      "0 -0.404751   0.507395 -0.918626   -0.083788 -0.306964   0.908842 -0.557611   \n",
      "1  0.699915   0.834941  0.922943   -0.083788 -0.306964  -0.626000 -0.557611   \n",
      "\n",
      "   sqft_living_log  sqft_lot_log  sqft_above_log  ...  has_view  is_luxury  \\\n",
      "0         0.339313      0.251916       -0.127334  ... -0.331514  -0.207647   \n",
      "1         1.003003     -0.242640        0.533037  ... -0.331514  -0.207647   \n",
      "\n",
      "   distance_from_seattle  price_log   price          id      lat     long  \\\n",
      "0              -0.545181  13.541074  760000  2579500006  47.5419 -122.214   \n",
      "1              -1.006131  13.017003  450000  7934000145  47.5563 -122.393   \n",
      "\n",
      "   image_exists                                         image_path  \n",
      "0          True  /Users/yashbansal/Documents/cdc/satellite_prop...  \n",
      "1          True  /Users/yashbansal/Documents/cdc/satellite_prop...  \n",
      "\n",
      "[2 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Explore Data\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv(os.path.join(config.PROCESSED_DIR, 'train_processed.csv'))\n",
    "val_df = pd.read_csv(os.path.join(config.PROCESSED_DIR, 'val_processed.csv'))\n",
    "test_df = pd.read_csv(os.path.join(config.PROCESSED_DIR, 'test_processed.csv'))\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"\\nTrain columns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nTest columns: {test_df.columns.tolist()}\")\n",
    "print(f\"\\nTrain head:\\n{train_df.head(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2885544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tabular features: 21\n",
      "Features: ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_living_log', 'sqft_lot_log', 'sqft_above_log', 'sqft_basement_log', 'sqft_living15_log', 'sqft_lot15_log', 'property_age', 'was_renovated', 'years_since_renovation', 'has_basement', 'total_rooms', 'has_view', 'is_luxury', 'distance_from_seattle']\n",
      "\n",
      "Target column: price_log\n",
      "Target stats - Min: 11.2252, Max: 15.8567, Mean: 13.0461\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define Feature Columns\n",
    "# Columns to exclude (metadata and target)\n",
    "EXCLUDE_COLS = ['id', 'lat', 'long', 'image_exists', 'image_path', 'price', 'price_log']\n",
    "\n",
    "# Get feature columns (all columns except excluded ones)\n",
    "TABULAR_FEATURES = [col for col in train_df.columns if col not in EXCLUDE_COLS]\n",
    "NUM_FEATURES = len(TABULAR_FEATURES)\n",
    "\n",
    "print(f\"Number of tabular features: {NUM_FEATURES}\")\n",
    "print(f\"Features: {TABULAR_FEATURES}\")\n",
    "\n",
    "# Target column - using price_log which is already computed\n",
    "TARGET_COL = 'price_log'\n",
    "print(f\"\\nTarget column: {TARGET_COL}\")\n",
    "print(f\"Target stats - Min: {train_df[TARGET_COL].min():.4f}, Max: {train_df[TARGET_COL].max():.4f}, Mean: {train_df[TARGET_COL].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1473343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms defined!\n",
      "Images will be resized from 1280x1280 to 224x224\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Image Transforms\n",
    "# Training transforms (with mild augmentation - preserve geography)\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((config.IMG_SIZE, config.IMG_SIZE)),  # Resize from 1280 to 224\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomApply([T.RandomRotation(degrees=[90, 90])], p=0.25),\n",
    "    T.RandomApply([T.RandomRotation(degrees=[180, 180])], p=0.25),\n",
    "    T.RandomApply([T.RandomRotation(degrees=[270, 270])], p=0.25),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.RandomApply([T.GaussianBlur(kernel_size=3)], p=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation/Test transforms (no augmentation)\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((config.IMG_SIZE, config.IMG_SIZE)),  # Resize from 1280 to 224\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Transforms defined!\")\n",
    "print(f\"Images will be resized from {config.ORIGINAL_IMG_SIZE}x{config.ORIGINAL_IMG_SIZE} to {config.IMG_SIZE}x{config.IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42242993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Dataset Class\n",
    "class MultiScalePropertyDataset(Dataset):\n",
    "    \"\"\"Dataset for multi-scale satellite imagery property valuation.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, img_dir_19, img_dir_20, features, target_col=None, transform=None, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir_19 = img_dir_19\n",
    "        self.img_dir_20 = img_dir_20\n",
    "        self.features = features\n",
    "        self.target_col = target_col\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Pre-compute valid indices (samples with both images)\n",
    "        self.valid_indices = []\n",
    "        missing_count = 0\n",
    "        \n",
    "        for idx in range(len(self.df)):\n",
    "            prop_id = self.df.loc[idx, 'id']\n",
    "            img_path_19 = os.path.join(self.img_dir_19, f\"{prop_id}_z19.jpg\")\n",
    "            img_path_20 = os.path.join(self.img_dir_20, f\"{prop_id}_z20.jpg\")\n",
    "            \n",
    "            if os.path.exists(img_path_19) and os.path.exists(img_path_20):\n",
    "                self.valid_indices.append(idx)\n",
    "            else:\n",
    "                missing_count += 1\n",
    "        \n",
    "        print(f\"Dataset initialized: {len(self.valid_indices)}/{len(self.df)} samples have both zoom images\")\n",
    "        if missing_count > 0:\n",
    "            print(f\"  Missing images for {missing_count} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.valid_indices[idx]\n",
    "        row = self.df.iloc[actual_idx]\n",
    "        prop_id = row['id']\n",
    "        \n",
    "        # Load images (naming: {id}_z19.jpg and {id}_z20.jpg)\n",
    "        img_path_19 = os.path.join(self.img_dir_19, f\"{prop_id}_z19.jpg\")\n",
    "        img_path_20 = os.path.join(self.img_dir_20, f\"{prop_id}_z20.jpg\")\n",
    "        \n",
    "        img_19 = Image.open(img_path_19).convert('RGB')\n",
    "        img_20 = Image.open(img_path_20).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            # Use same random seed for both images to apply same augmentation\n",
    "            seed = random.randint(0, 2**32)\n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            img_19 = self.transform(img_19)\n",
    "            random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "            img_20 = self.transform(img_20)\n",
    "        \n",
    "        # Tabular features\n",
    "        tabular = torch.tensor(row[self.features].values.astype(np.float32), dtype=torch.float32)\n",
    "        \n",
    "        # Property ID for tracking\n",
    "        prop_id_int = int(prop_id)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return img_19, img_20, tabular, prop_id_int\n",
    "        else:\n",
    "            # Target is price_log (already log-transformed)\n",
    "            target = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
    "            return img_19, img_20, tabular, target, prop_id_int\n",
    "\n",
    "print(\"Dataset class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9796655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized: 12967/12967 samples have both zoom images\n",
      "Dataset initialized: 3242/3242 samples have both zoom images\n",
      "Dataset initialized: 5404/5404 samples have both zoom images\n",
      "\n",
      "Train batches: 406\n",
      "Validation batches: 102\n",
      "Test batches: 169\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Create Datasets and DataLoaders\n",
    "# Training dataset - uses train_images_19 and train_images_20\n",
    "train_dataset = MultiScalePropertyDataset(\n",
    "    df=train_df,\n",
    "    img_dir_19=config.TRAIN_IMG_19,\n",
    "    img_dir_20=config.TRAIN_IMG_20,\n",
    "    features=TABULAR_FEATURES,\n",
    "    target_col=TARGET_COL,\n",
    "    transform=train_transform,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "# Validation dataset - also uses train_images_19 and train_images_20 (same source as train)\n",
    "val_dataset = MultiScalePropertyDataset(\n",
    "    df=val_df,\n",
    "    img_dir_19=config.TRAIN_IMG_19,\n",
    "    img_dir_20=config.TRAIN_IMG_20,\n",
    "    features=TABULAR_FEATURES,\n",
    "    target_col=TARGET_COL,\n",
    "    transform=val_transform,\n",
    "    is_test=False\n",
    ")\n",
    "\n",
    "# Test dataset - uses test_images_19 and test_images_20\n",
    "test_dataset = MultiScalePropertyDataset(\n",
    "    df=test_df,\n",
    "    img_dir_19=config.TEST_IMG_19,\n",
    "    img_dir_20=config.TEST_IMG_20,\n",
    "    features=TABULAR_FEATURES,\n",
    "    target_col=None,\n",
    "    transform=val_transform,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a16046ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TabularTransformer defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Tabular Transformer Encoder\n",
    "class TabularTransformer(nn.Module):\n",
    "    \"\"\"Transformer-based encoder for tabular features.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, d_model=128, nhead=4, num_layers=2, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Project each feature to d_model dimension\n",
    "        self.feature_embedding = nn.Linear(1, d_model)\n",
    "        \n",
    "        # Learnable positional encoding for each feature\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, num_features, d_model) * 0.02)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, output_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (B, num_features)\n",
    "        B = x.size(0)\n",
    "        \n",
    "        # Reshape to (B, num_features, 1) and embed each feature\n",
    "        x = x.unsqueeze(-1)  # (B, num_features, 1)\n",
    "        x = self.feature_embedding(x)  # (B, num_features, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = x + self.pos_encoding\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = self.transformer(x)  # (B, num_features, d_model)\n",
    "        \n",
    "        # Global average pooling over features\n",
    "        x = x.mean(dim=1)  # (B, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        x = self.output_proj(x)  # (B, output_dim)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"TabularTransformer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd68f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisualBackbone defined (75% frozen, 25% trainable)!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Visual Backbone (ResNet-50 with 25% unfreezing)\n",
    "class VisualBackbone(nn.Module):\n",
    "    \"\"\"ResNet50 backbone that extracts multi-scale features from layer3 and layer4.\n",
    "    Freezes 75% of the backbone, unfreezes 25% (layer3, layer4).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        # Extract layers\n",
    "        self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1  # 256 channels\n",
    "        self.layer2 = resnet.layer2  # 512 channels\n",
    "        self.layer3 = resnet.layer3  # 1024 channels, 14x14 for 224 input\n",
    "        self.layer4 = resnet.layer4  # 2048 channels, 7x7 for 224 input\n",
    "        \n",
    "        # Projection layers for multi-scale features\n",
    "        self.proj3 = nn.Sequential(\n",
    "            nn.Conv2d(1024, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.proj4 = nn.Sequential(\n",
    "            nn.Conv2d(2048, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final projection after concatenation\n",
    "        self.final_proj = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Freeze 75% of ResNet (conv1, bn1, layer1, layer2)\n",
    "        self._freeze_layers()\n",
    "    \n",
    "    def _freeze_layers(self):\n",
    "        \"\"\"Freeze conv1, bn1, layer1, layer2 (approximately 75% of backbone).\"\"\"\n",
    "        # Freeze these layers\n",
    "        frozen_layers = [self.conv1, self.bn1, self.layer1, self.layer2,self.layer3]\n",
    "        for layer in frozen_layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Keep layer3, layer4, and projection layers trainable (25%)\n",
    "        trainable_layers = [ self.layer4, self.proj3, self.proj4, self.final_proj]\n",
    "        for layer in trainable_layers:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layers (frozen)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # ResNet blocks\n",
    "        x = self.layer1(x)  # Frozen\n",
    "        x = self.layer2(x)  # Frozen\n",
    "        feat3 = self.layer3(x)  # Trainable - (B, 1024, 14, 14)\n",
    "        feat4 = self.layer4(feat3)  # Trainable - (B, 2048, 7, 7)\n",
    "        \n",
    "        # Project features\n",
    "        proj3 = self.proj3(feat3)  # (B, 256, 14, 14)\n",
    "        proj4 = self.proj4(feat4)  # (B, 256, 7, 7)\n",
    "        \n",
    "        # Upsample proj4 to match proj3 spatial size\n",
    "        proj4_up = F.interpolate(proj4, size=(14, 14), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Concatenate and project\n",
    "        combined = torch.cat([proj3, proj4_up], dim=1)  # (B, 512, 14, 14)\n",
    "        img_feats = self.final_proj(combined)  # (B, 256, 14, 14)\n",
    "        \n",
    "        return img_feats, feat4  # Return both for Grad-CAM\n",
    "\n",
    "print(\"VisualBackbone defined (75% frozen, 25% trainable)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1cd6181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossModalAttention defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Cross-Modal Attention Module\n",
    "class CrossModalAttention(nn.Module):\n",
    "    \"\"\"Cross-modal attention where tabular context queries image features.\"\"\"\n",
    "    \n",
    "    def __init__(self, tab_dim=128, img_dim=256, embed_dim=256, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Project tabular to query dimension\n",
    "        self.query_proj = nn.Linear(tab_dim, embed_dim)\n",
    "        \n",
    "        # Project image features to key/value dimension\n",
    "        self.key_proj = nn.Linear(img_dim, embed_dim)\n",
    "        self.value_proj = nn.Linear(img_dim, embed_dim)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer norm and output projection\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, tab_ctx, img_feats, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tab_ctx: (B, tab_dim) - tabular context vector\n",
    "            img_feats: (B, 256, 14, 14) - image feature maps\n",
    "            return_attention: whether to return attention weights\n",
    "        Returns:\n",
    "            attended: (B, embed_dim) - attended image features\n",
    "            attn_weights: (B, 1, 196) - attention weights if return_attention=True\n",
    "        \"\"\"\n",
    "        B = tab_ctx.size(0)\n",
    "        \n",
    "        # Flatten spatial dimensions: (B, 256, 14, 14) -> (B, 196, 256)\n",
    "        img_feats_flat = img_feats.flatten(2).permute(0, 2, 1)  # (B, 196, 256)\n",
    "        \n",
    "        # Project to query, key, value\n",
    "        Q = self.query_proj(tab_ctx).unsqueeze(1)  # (B, 1, embed_dim)\n",
    "        K = self.key_proj(img_feats_flat)  # (B, 196, embed_dim)\n",
    "        V = self.value_proj(img_feats_flat)  # (B, 196, embed_dim)\n",
    "        \n",
    "        # Cross-attention\n",
    "        attended, attn_weights = self.attention(Q, K, V, need_weights=True, average_attn_weights=True)\n",
    "        # attended: (B, 1, embed_dim), attn_weights: (B, 1, 196)\n",
    "        \n",
    "        # Squeeze and process\n",
    "        attended = attended.squeeze(1)  # (B, embed_dim)\n",
    "        attended = self.norm(attended)\n",
    "        attended = self.output_proj(attended)  # (B, embed_dim)\n",
    "        \n",
    "        if return_attention:\n",
    "            return attended, attn_weights\n",
    "        return attended, None\n",
    "\n",
    "print(\"CrossModalAttention defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9df3054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiScalePropertyModel defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Main Model\n",
    "class MultiScalePropertyModel(nn.Module):\n",
    "    \"\"\"Multi-scale property valuation model with cross-modal attention fusion.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Tabular encoder\n",
    "        self.tabular_encoder = TabularTransformer(\n",
    "            num_features=num_features,\n",
    "            d_model=config.TAB_D_MODEL,\n",
    "            nhead=config.TAB_NHEAD,\n",
    "            num_layers=config.TAB_NUM_LAYERS,\n",
    "            output_dim=config.TAB_OUTPUT_DIM\n",
    "        )\n",
    "        \n",
    "        # Shared visual backbone for both zoom levels\n",
    "        self.visual_backbone = VisualBackbone()\n",
    "        \n",
    "        # Cross-modal attention for each zoom level\n",
    "        self.cross_attn_19 = CrossModalAttention(\n",
    "            tab_dim=config.TAB_OUTPUT_DIM,\n",
    "            img_dim=config.ATTENTION_DIM,\n",
    "            embed_dim=config.ATTENTION_DIM,\n",
    "            num_heads=config.ATTENTION_HEADS\n",
    "        )\n",
    "        \n",
    "        self.cross_attn_20 = CrossModalAttention(\n",
    "            tab_dim=config.TAB_OUTPUT_DIM,\n",
    "            img_dim=config.ATTENTION_DIM,\n",
    "            embed_dim=config.ATTENTION_DIM,\n",
    "            num_heads=config.ATTENTION_HEADS\n",
    "        )\n",
    "        \n",
    "        # Project tabular context for fusion\n",
    "        self.tab_proj = nn.Linear(config.TAB_OUTPUT_DIM, config.ATTENTION_DIM)\n",
    "        \n",
    "        # Fusion MLP: [tab_ctx(256) + attended_19(256) + attended_20(256)] = 768\n",
    "        fusion_input_dim = config.ATTENTION_DIM * 3\n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, config.FUSION_HIDDEN[1]),\n",
    "            nn.LayerNorm(config.FUSION_HIDDEN[1]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.FUSION_HIDDEN[1], config.FUSION_HIDDEN[2]),\n",
    "            nn.LayerNorm(config.FUSION_HIDDEN[2]),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(config.FUSION_HIDDEN[2], 1)\n",
    "        )\n",
    "        \n",
    "        # Store intermediate features for Grad-CAM\n",
    "        self.feat4_19 = None\n",
    "        self.feat4_20 = None\n",
    "    \n",
    "    def forward(self, img_19, img_20, tabular, return_attention=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_19: (B, 3, 224, 224) - zoom 19 images\n",
    "            img_20: (B, 3, 224, 224) - zoom 20 images\n",
    "            tabular: (B, num_features) - tabular features\n",
    "            return_attention: whether to return attention weights\n",
    "        \"\"\"\n",
    "        # Encode tabular features\n",
    "        tab_ctx = self.tabular_encoder(tabular)  # (B, 128)\n",
    "        \n",
    "        # Extract visual features for both zoom levels (shared backbone)\n",
    "        img_feats_19, self.feat4_19 = self.visual_backbone(img_19)  # (B, 256, 14, 14)\n",
    "        img_feats_20, self.feat4_20 = self.visual_backbone(img_20)  # (B, 256, 14, 14)\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        attended_19, attn_19 = self.cross_attn_19(tab_ctx, img_feats_19, return_attention)\n",
    "        attended_20, attn_20 = self.cross_attn_20(tab_ctx, img_feats_20, return_attention)\n",
    "        \n",
    "        # Project tabular context\n",
    "        tab_ctx_proj = self.tab_proj(tab_ctx)  # (B, 256)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        fused = torch.cat([tab_ctx_proj, attended_19, attended_20], dim=1)  # (B, 768)\n",
    "        \n",
    "        # Predict log(price)\n",
    "        output = self.fusion_head(fused).squeeze(-1)  # (B,)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attn_19, attn_20\n",
    "        return output\n",
    "\n",
    "print(\"MultiScalePropertyModel defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5921ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 26,211,009\n",
      "Trainable parameters: 17,667,713 (67.4%)\n",
      "Frozen parameters: 8,543,296 (32.6%)\n",
      "\n",
      "Test forward pass:\n",
      "  Output shape: torch.Size([2])\n",
      "  Attention 19 shape: torch.Size([2, 1, 196])\n",
      "  Attention 20 shape: torch.Size([2, 1, 196])\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Initialize Model and Check Parameters\n",
    "model = MultiScalePropertyModel(num_features=NUM_FEATURES, config=config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "frozen_params = total_params - trainable_params\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "print(f\"Frozen parameters: {frozen_params:,} ({100*frozen_params/total_params:.1f}%)\")\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    test_img = torch.randn(2, 3, 224, 224).to(device)\n",
    "    test_tab = torch.randn(2, NUM_FEATURES).to(device)\n",
    "    out, attn_19, attn_20 = model(test_img, test_img, test_tab, return_attention=True)\n",
    "    print(f\"\\nTest forward pass:\")\n",
    "    print(f\"  Output shape: {out.shape}\")\n",
    "    print(f\"  Attention 19 shape: {attn_19.shape}\")\n",
    "    print(f\"  Attention 20 shape: {attn_20.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f124e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone params (lr=4e-05): 14,964,736\n",
      "Other params (lr=0.0003): 2,702,977\n",
      "Optimizer and scheduler configured!\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Loss Function, Optimizer, and Scheduler\n",
    "# Huber Loss on log(price)\n",
    "criterion = nn.HuberLoss(delta=config.HUBER_DELTA)\n",
    "\n",
    "# Separate parameter groups for different learning rates\n",
    "backbone_params = []\n",
    "other_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        if 'visual_backbone' in name and ('layer3' in name or 'layer4' in name):\n",
    "            backbone_params.append(param)\n",
    "        else:\n",
    "            other_params.append(param)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': config.LR_BACKBONE},\n",
    "    {'params': other_params, 'lr': config.LR_MAIN}\n",
    "], weight_decay=config.WEIGHT_DECAY)\n",
    "\n",
    "# Cosine annealing scheduler with warmup\n",
    "def get_lr_scheduler(optimizer, warmup_epochs, total_epochs, min_lr=1e-5):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            return max(min_lr / config.LR_MAIN, 0.5 * (1 + np.cos(np.pi * progress)))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "scheduler = get_lr_scheduler(optimizer, config.WARMUP_EPOCHS, config.EPOCHS)\n",
    "\n",
    "print(f\"Backbone params (lr={config.LR_BACKBONE}): {sum(p.numel() for p in backbone_params):,}\")\n",
    "print(f\"Other params (lr={config.LR_MAIN}): {sum(p.numel() for p in other_params):,}\")\n",
    "print(\"Optimizer and scheduler configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5edae967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Training Functions\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        img_19, img_20, tabular, targets, _ = batch\n",
    "        img_19 = img_19.to(device)\n",
    "        img_20 = img_20.to(device)\n",
    "        tabular = tabular.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(img_19, img_20, tabular)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(targets)\n",
    "        all_preds.extend(outputs.detach().cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    \n",
    "    # Calculate metrics on log scale\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    rmse_log = np.sqrt(np.mean((all_preds - all_targets) ** 2))\n",
    "    \n",
    "    # Calculate metrics on original scale (convert from log)\n",
    "    preds_orig = np.exp(all_preds)\n",
    "    targets_orig = np.exp(all_targets)\n",
    "    rmse_orig = np.sqrt(np.mean((preds_orig - targets_orig) ** 2))\n",
    "    r2 = 1 - np.sum((preds_orig - targets_orig) ** 2) / np.sum((targets_orig - np.mean(targets_orig)) ** 2)\n",
    "    \n",
    "    return avg_loss, rmse_log, rmse_orig, r2\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_ids = []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Validating\"):\n",
    "        img_19, img_20, tabular, targets, prop_ids = batch\n",
    "        img_19 = img_19.to(device)\n",
    "        img_20 = img_20.to(device)\n",
    "        tabular = tabular.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs = model(img_19, img_20, tabular)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        total_loss += loss.item() * len(targets)\n",
    "        all_preds.extend(outputs.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_ids.extend(prop_ids.numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    \n",
    "    # Calculate metrics on log scale\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    rmse_log = np.sqrt(np.mean((all_preds - all_targets) ** 2))\n",
    "    \n",
    "    # Calculate metrics on original scale (convert from log)\n",
    "    preds_orig = np.exp(all_preds)\n",
    "    targets_orig = np.exp(all_targets)\n",
    "    rmse_orig = np.sqrt(np.mean((preds_orig - targets_orig) ** 2))\n",
    "    r2 = 1 - np.sum((preds_orig - targets_orig) ** 2) / np.sum((targets_orig - np.mean(targets_orig)) ** 2)\n",
    "    \n",
    "    return avg_loss, rmse_log, rmse_orig, r2, all_preds, all_ids\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5467cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradCAM class defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Grad-CAM Implementation\n",
    "class GradCAM:\n",
    "    \"\"\"Grad-CAM implementation for ResNet layer4.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "    \n",
    "    def get_cam(self, input_img_19, input_img_20, tabular, zoom_level='19'):\n",
    "        \"\"\"Generate Grad-CAM heatmap for specified zoom level.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Enable gradients for input\n",
    "        input_img_19.requires_grad_(True)\n",
    "        input_img_20.requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(input_img_19, input_img_20, tabular)\n",
    "        \n",
    "        # Get the target feature map\n",
    "        if zoom_level == '19':\n",
    "            target_layer = self.model.feat4_19\n",
    "        else:\n",
    "            target_layer = self.model.feat4_20\n",
    "        \n",
    "        # Register hook for gradients\n",
    "        target_layer.retain_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.model.zero_grad()\n",
    "        output.backward(retain_graph=True)\n",
    "        \n",
    "        # Get gradients and activations\n",
    "        gradients = target_layer.grad  # (B, C, H, W)\n",
    "        activations = target_layer  # (B, C, H, W)\n",
    "        \n",
    "        # Pool gradients across spatial dimensions\n",
    "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)  # (B, C, 1, 1)\n",
    "        \n",
    "        # Weighted combination of activation maps\n",
    "        cam = torch.sum(weights * activations, dim=1)  # (B, H, W)\n",
    "        \n",
    "        # ReLU and normalize\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        if cam.max() > 0:\n",
    "            cam = cam / cam.max()\n",
    "        \n",
    "        return cam.detach().cpu().numpy()\n",
    "\n",
    "print(\"GradCAM class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a3452d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization functions defined!\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Visualization Functions\n",
    "def create_heatmap_overlay(img, heatmap, alpha=0.5):\n",
    "    \"\"\"Create heatmap overlay on image.\"\"\"\n",
    "    # Resize heatmap to match image\n",
    "    heatmap_resized = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "    \n",
    "    # Convert heatmap to colormap\n",
    "    heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n",
    "    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB) / 255.0\n",
    "    \n",
    "    # Overlay\n",
    "    overlay = alpha * heatmap_colored + (1 - alpha) * img\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "def visualize_explainability(model, img_19, img_20, tabular, prop_id, save_dir, prefix='val'):\n",
    "    \"\"\"Generate and save Grad-CAM and attention visualizations.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Clone tensors and enable gradients for Grad-CAM\n",
    "    img_19_input = img_19.clone().requires_grad_(True)\n",
    "    img_20_input = img_20.clone().requires_grad_(True)\n",
    "    \n",
    "    # Forward pass with attention\n",
    "    output, attn_19, attn_20 = model(img_19_input, img_20_input, tabular, return_attention=True)\n",
    "    \n",
    "    # Get attention maps (reshape from (B, 1, 196) to (14, 14))\n",
    "    attn_map_19 = attn_19.squeeze(1).reshape(-1, 14, 14).detach().cpu().numpy()[0]\n",
    "    attn_map_20 = attn_20.squeeze(1).reshape(-1, 14, 14).detach().cpu().numpy()[0]\n",
    "    \n",
    "    # Normalize attention maps\n",
    "    attn_map_19 = (attn_map_19 - attn_map_19.min()) / (attn_map_19.max() - attn_map_19.min() + 1e-8)\n",
    "    attn_map_20 = (attn_map_20 - attn_map_20.min()) / (attn_map_20.max() - attn_map_20.min() + 1e-8)\n",
    "    \n",
    "    # Get Grad-CAM for zoom 19\n",
    "    gradcam = GradCAM(model)\n",
    "    img_19_grad = img_19.clone().requires_grad_(True)\n",
    "    img_20_grad = img_20.clone().requires_grad_(True)\n",
    "    cam_19 = gradcam.get_cam(img_19_grad, img_20_grad, tabular, zoom_level='19')[0]\n",
    "    \n",
    "    # Get Grad-CAM for zoom 20\n",
    "    img_19_grad2 = img_19.clone().requires_grad_(True)\n",
    "    img_20_grad2 = img_20.clone().requires_grad_(True)\n",
    "    cam_20 = gradcam.get_cam(img_19_grad2, img_20_grad2, tabular, zoom_level='20')[0]\n",
    "    \n",
    "    # Convert images to numpy for visualization (denormalize)\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    img_19_np = img_19.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    img_19_np = std * img_19_np + mean\n",
    "    img_19_np = np.clip(img_19_np, 0, 1)\n",
    "    \n",
    "    img_20_np = img_20.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "    img_20_np = std * img_20_np + mean\n",
    "    img_20_np = np.clip(img_20_np, 0, 1)\n",
    "    \n",
    "    # Create overlays\n",
    "    gradcam_overlay_19 = create_heatmap_overlay(img_19_np, cam_19)\n",
    "    attn_overlay_19 = create_heatmap_overlay(img_19_np, attn_map_19)\n",
    "    gradcam_overlay_20 = create_heatmap_overlay(img_20_np, cam_20)\n",
    "    attn_overlay_20 = create_heatmap_overlay(img_20_np, attn_map_20)\n",
    "    \n",
    "    # Create figure: 2 rows (zoom levels) x 3 cols (original, gradcam, attention)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Zoom 19\n",
    "    axes[0, 0].imshow(img_19_np)\n",
    "    axes[0, 0].set_title('Zoom 19 - Original')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(gradcam_overlay_19)\n",
    "    axes[0, 1].set_title('Zoom 19 - Grad-CAM')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(attn_overlay_19)\n",
    "    axes[0, 2].set_title('Zoom 19 - Attention')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # Zoom 20\n",
    "    axes[1, 0].imshow(img_20_np)\n",
    "    axes[1, 0].set_title('Zoom 20 - Original')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(gradcam_overlay_20)\n",
    "    axes[1, 1].set_title('Zoom 20 - Grad-CAM')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(attn_overlay_20)\n",
    "    axes[1, 2].set_title('Zoom 20 - Attention')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    pred_price = np.exp(output.item())\n",
    "    fig.suptitle(f'Property ID: {prop_id} | Predicted Price: ${pred_price:,.0f}', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f'{prefix}_{prop_id}_explainability.png')\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "print(\"Visualization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a7abef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 40 epochs...\n",
      "Early stopping patience: 15\n",
      "============================================================\n",
      "\n",
      "Epoch 1/40 | LR: 6.00e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b7c1527af643b094747cebd1839794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0fd898ebdc460e9e549baaf7824ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train - Loss: 8.2224, RMSE: $651,301, R²: -2.1670\n",
      "  Val   - Loss: 7.0126, RMSE: $629,243, R²: -2.4961\n",
      "  ✓ New best model saved! RMSE: $629,243\n",
      "\n",
      "Epoch 2/40 | LR: 1.20e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1de53c2457a427986adc48caea7b9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ccd32ba91e7471d8d9c44eca5df34bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train - Loss: 4.7609, RMSE: $645,641, R²: -2.1122\n",
      "  Val   - Loss: 2.2041, RMSE: $603,593, R²: -2.2169\n",
      "  ✓ New best model saved! RMSE: $603,593\n",
      "\n",
      "Epoch 3/40 | LR: 1.80e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b75b439d4894d768f2697009199303d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0247bd903e4e43a1b16661855697f58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train - Loss: 0.7139, RMSE: $628,465, R²: -1.9488\n",
      "  Val   - Loss: 0.1299, RMSE: $352,526, R²: -0.0973\n",
      "  ✓ New best model saved! RMSE: $352,526\n",
      "\n",
      "Epoch 4/40 | LR: 2.40e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3b258b4e3f470892cfc2aa492c5815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8715559d82104a5f82cc261f20eefd4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train - Loss: 0.4525, RMSE: $712,223, R²: -2.7872\n",
      "  Val   - Loss: 0.0864, RMSE: $305,982, R²: 0.1733\n",
      "  ✓ New best model saved! RMSE: $305,982\n",
      "\n",
      "Epoch 5/40 | LR: 3.00e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0c054d5b9e485598be2996a17e4726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/406 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0802de8b632409a99775db7a3da5038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 17: Training Loop\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_rmse_log': [], 'val_rmse_log': [],\n",
    "    'train_rmse': [], 'val_rmse': [],\n",
    "    'train_r2': [], 'val_r2': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "best_val_rmse = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "print(f\"Starting training for {config.EPOCHS} epochs...\")\n",
    "print(f\"Early stopping patience: {config.EARLY_STOPPING_PATIENCE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    current_lr = optimizer.param_groups[1]['lr']  # Main LR\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.EPOCHS} | LR: {current_lr:.2e}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_rmse_log, train_rmse, train_r2 = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_rmse_log, val_rmse, val_r2, _, _ = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_rmse_log'].append(train_rmse_log)\n",
    "    history['val_rmse_log'].append(val_rmse_log)\n",
    "    history['train_rmse'].append(train_rmse)\n",
    "    history['val_rmse'].append(val_rmse)\n",
    "    history['train_r2'].append(train_r2)\n",
    "    history['val_r2'].append(val_r2)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"  Train - Loss: {train_loss:.4f}, RMSE: ${train_rmse:,.0f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"  Val   - Loss: {val_loss:.4f}, RMSE: ${val_rmse:,.0f}, R²: {val_r2:.4f}\")\n",
    "    \n",
    "    # Check for improvement (using RMSE on original price scale)\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_rmse': val_rmse,\n",
    "            'val_r2': val_r2,\n",
    "            'history': history\n",
    "        }, config.MODEL_SAVE_PATH)\n",
    "        print(f\"  ✓ New best model saved! RMSE: ${best_val_rmse:,.0f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement. Patience: {patience_counter}/{config.EARLY_STOPPING_PATIENCE}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training complete! Best validation RMSE: ${best_val_rmse:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab65bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
